# stats/comparison.py
"""
This module is responsible for comparing the statistical summaries of two
different Reddit users or two different sets of filtered data for the same user
(e.g., comparing stats from different time periods).

It takes the pre-calculated statistics dictionaries (generated by the
calculations.py module) for each user/dataset and performs specific comparisons
like subreddit overlap and common word frequency.

Finally, it orchestrates the formatting and saving of a human-readable
comparison report.
"""

import logging
import os
import time
from collections import Counter # Used for potential counting logic, though mainly in calculations.py, kept for clarity
from datetime import datetime # Used potentially for timestamp handling, though main logic is in calculations.py, kept for clarity

# --- Import from sibling module ---
# _format_comparison_report is a function from the 'reporting' module (likely in the same package)
# that takes the calculated stats and comparison results and formats them into
# a final report string (e.g., Markdown or text).
# CYAN and RESET are color constants from core_utils for terminal logging output.
from .reporting import _format_comparison_report
from .core_utils import CYAN, RESET, BOLD, RED # Added BOLD, RED for error logging


# --- Comparison Calculation Helpers ---
# These functions perform specific comparison calculations based on the
# pre-calculated stats passed to them. They are prefixed with '_' to indicate
# they are internal helpers used by generate_comparison_report.

def _calculate_subreddit_overlap(subs1, subs2):
    """
    Calculates the overlap between two lists or sets of subreddits.
    This indicates how many subreddits both users (or both datasets)
    were active in, and provides a Jaccard Index score as a measure
    of similarity (shared items / total unique items).

    Args:
        subs1 (list or set): A list or set of subreddit names for the first user/dataset.
                             Expected to be the 'all_active_subs' list from temporal stats.
        subs2 (list or set): A list or set of subreddit names for the second user/dataset.
                             Expected to be the 'all_active_subs' list from temporal stats.

    Returns:
        dict: A dictionary containing comparison results:
              - "shared_subreddits": A sorted list of subreddit names present in both inputs.
              - "num_shared": The total count of shared subreddits.
              - "jaccard_index": A string representing the Jaccard similarity index (0.0 to 1.0), formatted to 3 decimal places.
                                 0.0 means no overlap, 1.0 means identical sets.
    """
    logging.debug("         Calculating subreddit overlap...")

    # Ensure inputs are sets for efficient intersection and union operations.
    # Handle cases where inputs might be None or empty lists.
    set1 = set(subs1) if subs1 else set()
    set2 = set(subs2) if subs2 else set()

    # Calculate the intersection (elements present in both sets)
    intersection = set1.intersection(set2)

    # Calculate the union (all unique elements from both sets combined)
    union = set1.union(set2)

    # Calculate the Jaccard index: |Intersection| / |Union|
    # This measures the similarity between the two sets.
    # Protect against division by zero if the union is empty (no subs in either list).
    jaccard = len(intersection) / len(union) if union else 0.0

    return { "shared_subreddits": sorted(list(intersection), key=str.lower), # Return shared subs as a sorted list
             "num_shared": len(intersection),
             "jaccard_index": f"{jaccard:.3f}" } # Format index as a string

def _compare_word_frequency(freq1, freq2, top_n=20):
    """
    Compares the most frequently used words between two users/datasets.
    It takes the top N words from each frequency list and calculates
    the overlap (shared words) and a Jaccard index for similarity
    among *only* those top words.

    Args:
        freq1 (dict): A dictionary mapping words to their frequency counts for the first user/dataset.
                      Expected to be the 'word_frequency' dictionary from text stats.
        freq2 (dict): A dictionary mapping words to their frequency counts for the second user/dataset.
                      Expected to be the 'word_frequency' dictionary from text stats.
        top_n (int): The number of top words to consider from each frequency dictionary for the comparison.

    Returns:
        dict: A dictionary containing comparison results:
              - "top_n_compared": The value of N used for this comparison.
              - "shared_top_words": A sorted list of words that appear in the top N of both inputs.
              - "num_shared_top_words": The total count of shared top words.
              - "jaccard_index": A string representing the Jaccard similarity index (0.0 to 1.0) based on the top N words, formatted to 3 decimal places.
    """
    # Log the comparison based on the requested N
    logging.debug(f"         Comparing top {top_n} word frequencies...")

    # Ensure inputs are dictionaries, defaulting to empty dicts if not valid.
    # This prevents errors if the preceding calculation (_calculate_word_frequency) failed or returned unexpected data.
    if not isinstance(freq1, dict): freq1 = {}
    if not isinstance(freq2, dict): freq2 = {}

    # Extract the words themselves from the frequency dictionaries' keys.
    # Assumes the frequency dictionary is ordered by frequency (e.g., from Counter.most_common).
    # Take only the top_n words.
    top_words1 = set(list(freq1.keys())[:top_n])
    top_words2 = set(list(freq2.keys())[:top_n])

    # Calculate intersection and union of these top word sets.
    intersection = top_words1.intersection(top_words2)
    union = top_words1.union(top_words2)

    # Calculate the Jaccard index for the top words.
    # Protect against division by zero if the union is empty (neither list had any words).
    jaccard = len(intersection) / len(union) if union else 0.0

    return { "top_n_compared": top_n,
             "shared_top_words": sorted(list(intersection)), # Return shared words as a sorted list
             "num_shared_top_words": len(intersection),
             "jaccard_index": f"{jaccard:.3f}" } # Format index as a string


# --- Main Comparison Report Generation Function ---

def generate_comparison_report(stats1, stats2, user1, user2, output_path):
    """
    Generates and saves a comparison report based on two dictionaries
    containing pre-calculated user/dataset statistics. This function
    orchestrates the comparison calculations and report formatting.

    Args:
        stats1 (dict): The dictionary containing pre-calculated statistics for the first user/dataset.
                       This dict is expected to have keys like 'subreddit_activity', 'word_frequency', etc.,
                       matching the output of the calculations.py module.
        stats2 (dict): The dictionary containing pre-calculated statistics for the second user/dataset.
                       Same structure as stats1.
        user1 (str): The username or identifier for the first set of stats (used in the report title/labels).
        user2 (str): The username or identifier for the second set of stats (used in the report title/labels).
        output_path (str): The full file path (including directory and filename) where the comparison report
                           should be saved (e.g., 'output/comparison_report_user1_vs_user2.md').

    Returns:
        bool: True if the comparison report was successfully generated and saved, False otherwise.
    """
    # Log the start of the comparison process
    logging.info(f"   👥 Generating comparison report for {BOLD}/u/{user1}{RESET} vs {BOLD}/u/{user2}{RESET}...")
    start_time = time.time() # Record start time for performance measurement

    # --- Input Validation ---
    # Check if the essential inputs are valid before proceeding.
    if not stats1 or not isinstance(stats1, dict):
        logging.error(f"   {BOLD}{RED}❌ Cannot generate comparison report: Invalid or missing stats data for user {user1}.{RESET}")
        return False
    if not stats2 or not isinstance(stats2, dict):
        logging.error(f"   {BOLD}{RED}❌ Cannot generate comparison report: Invalid or missing stats data for user {user2}.{RESET}")
        return False
    if not output_path:
         logging.error(f"   {BOLD}{RED}❌ Cannot save comparison report: No output path provided.{RESET}")
         return False

    # Dictionary to store the results of the comparison calculations
    comparison_results = {}
    report_content = "" # Variable to hold the final formatted report text
    success = False # Flag to track overall success status

    try:
        logging.info("      Calculating comparison metrics...")

        # --- Calculate Subreddit Overlap ---
        # Retrieve the list of all active subreddits from the input stats dictionaries.
        # Use .get() with default empty list [] for safety if keys are missing.
        subs1 = stats1.get('subreddit_activity', {}).get('all_active_subs', [])
        subs2 = stats2.get('subreddit_activity', {}).get('all_active_subs', [])
        # Call the helper function and store its result.
        comparison_results["subreddit_overlap"] = _calculate_subreddit_overlap(subs1, subs2)
        logging.debug(f"         Subreddit overlap calculated: {comparison_results['subreddit_overlap']['num_shared']} shared subs.")


        # --- Calculate Word Frequency Overlap ---
        # Retrieve the word frequency dictionaries from the input stats dictionaries.
        # The structure is assumed to be stats_dict -> 'word_frequency' -> 'word_frequency' dict.
        freq1 = stats1.get('word_frequency', {}).get('word_frequency', {})
        freq2 = stats2.get('word_frequency', {}).get('word_frequency', {})
        # Call the helper function to compare the top N words.
        # A larger N (e.g., 150) is used here for the *calculation* to get a more
        # representative Jaccard index of common words across their broader usage patterns.
        # The report formatting function will decide which specific top words to *display*.
        comparison_results["word_frequency_comparison"] = _compare_word_frequency(freq1, freq2, top_n=150)
        logging.debug(f"         Word frequency overlap calculated: {comparison_results['word_frequency_comparison']['num_shared_top_words']} shared top words.")

        # --- N-gram comparison ---
        # Note: N-gram comparison (like shared bigrams) is often best handled
        # directly within the formatting function (_format_comparison_report)
        # because it might involve iterating through the top N-grams from
        # the original stats dictionaries rather than a single aggregated index.
        # No separate calculation function is called here; the stats1 and stats2
        # dictionaries containing N-gram data are passed directly to the formatter.
        logging.debug("         N-gram comparison will be handled during report formatting.")


        logging.info("   ✍️ Formatting comparison report...")
        # Call the formatting function from the 'reporting' module.
        # It takes the raw stats for both users, their names, and the
        # specific comparison results calculated above to assemble the final text.
        report_content = _format_comparison_report(stats1, stats2, user1, user2, comparison_results)

        logging.info(f"   💾 Saving comparison report to {CYAN}{output_path}{RESET}...")
        # Ensure the output directory exists. The `os.makedirs` call with `exist_ok=True`
        # will create the directory if it doesn't exist and won't raise an error if it does.
        # os.path.dirname() gets the directory part of the path. '.' means current directory if output_path is just a filename.
        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
        # Open the specified output file in write mode ('w') with UTF-8 encoding.
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content) # Write the formatted report content to the file

        # If we reached here, the process was successful.
        elapsed_time = time.time() - start_time # Calculate elapsed time
        logging.info(f"   ✅ Comparison report saved successfully to {CYAN}{output_path}{RESET} ({elapsed_time:.2f}s).")
        success = True # Set success flag to True

    except Exception as e:
        # Catch any unexpected errors during the process (calculation, formatting, saving)
        logging.error(f"   {BOLD}{RED}❌ Error during comparison report generation for {user1} vs {user2}: {e}{RESET}", exc_info=True)
        success = False # Ensure success flag is False on exception

    return success # Return the final success status