![image](https://github.com/user-attachments/assets/18cf5aec-faf4-4c55-8935-5f027ad37f05)
***

# RedStalk v1.9.6 üïµÔ∏è‚Äç‚ôÄÔ∏è - Advanced Reddit User Activity Analysis

<sub> this whole readme.md was generated by gemini-2.5-pro and detailed and humanized by 2.5-flash, combined with around 60% to 70% of the code in here. please don't blame us for this. this was supposed to be a thursday project üòÖ </sub>

So, **RedStalk**? It's basically a pretty slick command-line tool built to do some serious analysis on public Reddit user activity. It gets the data by smartly scraping stuff, letting you filter things out, running statistical analysis, and even bringing in some advanced AI capabilities. The goal? To give you deep insights into how users behave, what they're interested in, their communication style, sentiment ‚Äì all that stuff, just from their publicly available posts and comments.

Whether you're looking into online communities for research, a data analyst checking out user trends, or just kinda curious about what a specific user puts out there publicly, RedStalk gives you a powerful set of tools to explore and understand those Reddit interactions.

<sub> while searching for redstalk on google and bing, i came across</sub> [the og redstalk](https://www.redstalk.net/)<sub> and i absolutely fw their art and design. check them out, sorry for using the name - it was too late when i realized üôè</sub>

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-AGPLv3-orange.svg)](https://opensource.org/license/agpl-v3)
[![GitHub Repository](https://img.shields.io/badge/GitHub-redstalk-brightgreen)](https://github.com/sarthak-sidhant/redstalk)

## Table of Contents

1.  [Overview](#overview)
2.  [Features](#features) ‚ú®
3.  [Installation](#installation)
    * [Prerequisites](#prerequisites)
    * [Cloning the Repository](#cloning-the-repository)
    * [Virtual Environment (Recommended)](#virtual-environment-recommended)
    * [Installing Dependencies](#installing-dependencies)
4.  [Configuration](#configuration) ‚öôÔ∏è
    * [Key `config.json` Settings](#key-configjson-settings)
    * [Setting your Google Gemini API Key](#setting-your-google-gemini-api-key)
    * [Setting your Reddit User-Agent](#setting-your-reddit-user-agent)
    * [Resetting Configuration](#resetting-configuration)
5.  [Usage](#usage) ‚ñ∂Ô∏è
    * [Basic Syntax](#basic-syntax)
    * [Getting Help](#getting-help)
    * [Core Actions Explained](#core-actions-explained)
        * [Single User Processing](#single-user-processing)
        * [Comparing Two Users](#comparing-two-users)
        * [Monitoring a User](#monitoring-a-user)
        * [Generating AI Prompts](#generating-ai-prompts)
        * [Exporting JSON Data Only](#exporting-json-data-only)
    * [Examples](#examples) üöÄ
    * [Key Options Detailed](#key-options-detailed)
6.  [Statistical Analysis (`stats` module)](#statistical-analysis-stats-module) üìä
    * [Available Metrics](#available-metrics)
7.  [AI-Powered Analysis (`analysis` module)](#ai-powered-analysis-analysis-module) ü§ñ
    * [Analysis Modes (`mapped` vs `raw`)](#analysis-modes-mapped-vs-raw)
    * [Prompt Engineering](#prompt-engineering)
8.  [Output Files](#output-files) üìÇ
9.  [How It Works (High Level Architecture)](#how-it-works-high-level-architecture) üèóÔ∏è
10. [Dependencies](#dependencies) üîó
11. [Contributing](#contributing) ü§ó
12. [License](#license) üìú
13. [Disclaimer and Ethical Considerations](#disclaimer-and-ethical-considerations) ‚ö†Ô∏è
14. [Troubleshooting](#troubleshooting) ü§î
15. [Future Enhancements](#future-enhancements) ‚ú®üîÆ

## Overview

RedStalk basically gives you this whole workflow for analyzing Reddit user profiles, start to finish. It begins by grabbing raw data using the Reddit API, then moves on to filtering, processing, and analyzing everything. It uses both traditional statistical methods *and* some fancy natural language processing powered by Google's Gemini AI. The whole point is to take a user's public activity stream and turn it into structured insights and easy-to-read summaries.

The tool's built to be pretty efficient too. It does incremental scraping, meaning it just updates existing datasets by only grabbing new activity since your last save. This seriously speeds things up on subsequent runs. Plus, it has flexible filtering options so you can really zero in on what you want to analyze. All the results get saved neatly into organized, human-readable files (Markdown, CSV, JSON) so they're easy to check out or use somewhere else.

## Features ‚ú®

Here's a rundown of what RedStalk can do:

*   **Efficient Data Scraping:** Grabs user posts and comments from the Reddit API. It's smart about updating existing data files ‚Äì it finds the newest stuff you already have saved and only asks Reddit for activity newer than that. This makes running it again way faster! ~~You can also tell it to grab *only* comments (`--scrape-comments-only`)~~ (im not sure if it works) or make it do a full re-scrape of everything (`--force-scrape`).
*   **Granular Data Filtering:** Lets you be super specific about what data makes it into the analysis.
    *   **Date Range:** Filter activity just between specific start and end dates (`--start-date`, `--end-date`).
    *   **Subreddit Inclusion:** Want to *only* look at stuff in certain subreddits? Use `--focus-subreddit`.
    *   **Subreddit Exclusion:** Or maybe you want to ignore posts/comments from specific subreddits? That's `--ignore-subreddit`. You can mix and match these filters too.
*   **Multiple Output Formats:** Gives you data and reports in different flavors:
    *   **Raw Data:** Saves all the scraped posts and comments as `.json` files (one file per user).
    *   **Filtered Data:** Takes that filtered dataset (after applying your date/subreddit rules) and puts it into separate `.csv` files for posts and comments. Super handy for pulling into spreadsheets or other analysis tools.
    *   **Statistical Reports:** Generates detailed, structured reports in `.md` (Markdown) format with tons of quantitative metrics.
    *   **AI Analysis Reports:** Creates those narrative summaries, character profiles, or whatever else the AI comes up with, also in `.md` format.
    *   **Comparison Reports:** Makes cool side-by-side statistical comparisons of two users, also in `.md`.
    *   **Optional JSON Stats Export:** You can choose to save the raw statistical data the `stats` module calculated into a `.json` file too ‚Äì useful if you want to use that data with other scripts or programs.
*   **Comprehensive Statistical Analysis:** (You'll need the `stats` dependencies for this) Calculates a whole bunch of metrics, like:
    *   Activity Timelines (when they post/comment)
    *   Subreddit Engagement (Diversity, Frequency, Top Subs)
    *   Content Characteristics (Word Counts, Lexical Diversity, even an estimated Question Ratio, Post/Comment Type breakdown)
    *   Engagement Scores (Upvotes, Awards, Comments per Post)
    *   Metadata Insights (Flair usage, Editing frequency, Estimates on deleted content)
    *   Simple NLP Metrics (Most common Words/N-grams, Sentiment Analysis using VADER)
*   **AI-Powered Deep Analysis:** (Needs that Google Gemini API Key) Uses the Gemini API to do qualitative analysis on the actual text from the user's activity:
    *   **Custom Prompts:** The analysis is driven by a system prompt you give it ‚Äì you can tell the AI exactly what kind of task to do (like "create a personality profile," "summarize opinions on this topic," "analyze writing style"). You load these prompts from files (`--prompt-file`).
    *   **Analysis Modes:** Pick between `mapped` mode (which groups comments under their parent posts to give the AI context) and `raw` mode (just gives the AI a chronological list of posts and comments).
    *   **External Context Fetching:** In `mapped` mode, you can optionally tell it to grab the titles for any external URLs the user posted. This helps the AI understand the context of the links (`--fetch-external-context`).
    *   **Token Handling:** It's smart about handling lots of text. It breaks the data into smaller chunks so it fits within the AI model's token limits.
    *   **AI Stats Summary:** There's an option to have the AI write a summary of the *statistical* report (`--summarize-stats`).
*   **User Comparison:** Has a specific mode (`--compare-user`) just for creating a single Markdown report that puts the statistical profiles of two users right next to each other.
*   **Real-time Monitoring:** Includes a background mode (`--monitor`) that just periodically checks a user to see if they've posted or commented anything new. If it finds new stuff, it automatically scrapes it, updates your JSON file, and exports the updated CSVs. (Could potentially trigger analysis on update later, maybe!)
*   **Interactive Prompt Engineering:** There's a little guided tool right in the command line (`--generate-prompt`) to help you write really effective and specific system prompts for the AI analysis module.
*   **Flexible Configuration:** You can easily manage default settings in a `config.json` file, or just override them on the fly using command-line flags. It also supports keeping your API key safe using an environment variable.
*   **Enhanced Logging:** Uses cool colored console output (thanks `rich` library!) so you can easily see what the tool is doing, how it's going, and if anything goes wrong.

## Installation

Want to get RedStalk running? Here's how you do it.

### Prerequisites

*   **Python 3.8 or higher:** Grab it from [python.org](https://www.python.org/downloads/).
*   **`pip`:** That's Python's package installer (usually comes with Python these days).
*   **`git`:** You need this to clone the repository. Get it from [git-scm.com](https://git-scm.com/downloads).

### Cloning the Repository

Open up your terminal or command prompt. Time for some cloning:

```bash
git clone https://github.com/sarthak-sidhant/redstalk.git
cd redstalk
```

### Virtual Environment (Recommended)

Seriously, using a virtual environment is a good idea. It keeps RedStalk's dependencies separate from all your other Python stuff.

```bash
# Make a virtual environment, calling it 'venv'
python -m venv venv

# Now, activate it:

# On Windows:
venv\Scripts\activate

# On macOS/Linux:
source venv/bin/activate
```

You'll know it's active because you'll see `(venv)` at the start of your terminal prompt.

### Installing Dependencies

You need a `requirements.txt` file in the root directory of the repo you just cloned (right next to `redstalk.py`). Paste this inside:

```txt
# Core stuff for scraping and files
requests>=2.25.0
rich>=10.0.0 # Makes the logging look nice with colors and progress bars

# Optional stuff - Install only if you plan to use these features

# Need this for all AI features (--run-analysis, --summarize-stats, --generate-prompt)
google-generativeai>=0.3.0

# Need this for Statistical Analysis (--generate-stats, --compare-user)
vaderSentiment>=3.3.0       # For that VADER sentiment analysis
pandas>=1.2.0               # Helps with data handling in some calculations

# Optional, but good for seeing progress during AI analysis
tqdm>=4.60.0 # Might get installed automatically by google-generativeai anyway, but listing it clearly
```

Okay, now install everything using pip:

```bash
pip install -r requirements.txt
```

## Configuration ‚öôÔ∏è

RedStalk uses a `config.json` file for its default settings. There might be one included already. If not, just create one in the main project directory.

**Here's what an example `config.json` might look like:**

```json
{
    "default_output_dir": "data",
    "default_prompt_file": "default_prompt.txt",
    "default_prompt_dir": "prompts",
    "default_chunk_size": 1000000,
    "api_key": "put in your own",
    "default_model_name": "gemini-2.0-flash",
    "monitor_interval_seconds": 300,
    "user_agent": "Python:RedStalkScript:v1.9 (by /u/YourRedditUsername)"
}
```

### Key `config.json` Settings

  * **`default_output_dir`**: This is where all the user data files (JSON, CSVs, reports) will go by default. It usually makes subdirectories for each username inside this one. Default: `"data"`.
  * **`default_prompt_file`**: If you use `--run-analysis` without specifying a prompt file, it'll look for this file (inside the `default_prompt_dir`). Default: `"default_prompt.txt"`.
  * **`default_prompt_dir`**: The directory where you keep your AI prompt files and where the interactive prompt generator saves new ones. Default: `"prompts"`.
  * **`default_chunk_size`**: This is roughly the target size in tokens for the chunks of data sent to the Gemini API for AI analysis. The actual size can be a little different. A smaller size might be better for models with smaller context windows or more detailed analysis, while bigger sizes can process more text at once. Note: `1000000` in the example works well for models like `gemini-2.0-flash` which have huge context windows. Adjust this based on the model you pick and what you're trying to do. Default: `1000000`.
  * **`api_key`**: **Your Google Gemini API Key.** You *can* put it here, but it's **way more secure** to use the `GOOGLE_API_KEY` environment variable instead.
  * **`default_model_name`**: Which Google Gemini model you want to use for AI analysis. Models with big context windows like `gemini-2.0-flash` or `gemini-1.5-pro` are recommended. Make sure the model supports your chosen chunk size. Default: `"gemini-2.0-flash"`.
  * **`monitor_interval_seconds`**: The default time (in seconds) it waits between checks when you're using the `--monitor` mode. Default: `300` (that's 5 minutes).
  *   **`user_agent`**: **THIS IS SUPER IMPORTANT!** üö® **You ABSOLUTELY MUST set a unique and descriptive User-Agent string here, and it HAS to include your Reddit username.** This isn't optional, it's a requirement from the Reddit API Terms of Service. Swap out `/u/YourRedditUsername` with your actual Reddit username. Example: `"MyRedditAnalysisBot/1.0 (by /u/YourUsername)"`. If you don't set a proper User-Agent, Reddit might limit your requests or block you entirely.

### Setting your Google Gemini API Key

You need this key for anything AI-related (`--run-analysis`, `--summarize-stats`, `--generate-prompt`). **Using an environment variable is the best and most secure way to do it.**

1.  **Get your Key:** Grab your key from [Google AI Studio](https://aistudio.google.com/app/apikey) or the Google Cloud Console.
2.  **Set Environment Variable (Recommended):** Set an environment variable called `GOOGLE_API_KEY` to your key.
      * **On Windows:**
        ```cmd
        set GOOGLE_API_KEY=YOUR_API_KEY_HERE
        ```
        (Want it permanent? Look up how to set System Properties -> Environment Variables.)
      * **On macOS/Linux:**
        ```bash
        export GOOGLE_API_KEY='YOUR_API_KEY_HERE'
        ```
        (To make it permanent, add this line to your shell's profile file, like `~/.bashrc` or `~/.zshrc`.)
3.  **Set in `config.json` (Less Secure):** If you really can't use an environment variable, you can paste your key into the `api_key` spot in `config.json`.
4.  **Set via Command Line (Least Secure, Highest Priority):** You *can* just put the key right after the `--api-key` flag. Don't recommend this for regular use, but it works for testing.

RedStalk looks for the API key in this order: Command Line Flag > Environment Variable > `config.json`. It uses the first one it finds.

### Setting your Reddit User-Agent

Like we said before, setting a proper User-Agent is **required** and **super important** to follow Reddit's rules. Go into your `config.json` and change `"by /u/YourRedditUsername"` to your *actual* Reddit username. This helps Reddit know who's using the API and reach out if they need to.

### Resetting Configuration

Oops, mess up your `config.json`? Or just want the default settings that came with the script? Use the `--reset-config` flag:

```bash
python redstalk.py --reset-config
```

This will overwrite your current `config.json` file with the default values built into the script.

## Usage ‚ñ∂Ô∏è

RedStalk is a command-line app. You run it by typing `python redstalk.py`, usually followed by the target username (if you're doing single-user stuff) and then whatever action flags and options you want.

### Basic Syntax

```bash
python redstalk.py [USERNAME] [ACTIONS...] [OPTIONS...]
```

Or, for things that don't start with a specific user (like comparing or making prompts):

```bash
python redstalk.py [EXCLUSIVE_ACTION] [OPTIONS...]
```

### Getting Help

Need to see all the commands, options, and what they do? Just ask for help:

```bash
python redstalk.py --help
# or shorthand
python redstalk.py -h
```

### Core Actions Explained

You'll usually tell RedStalk to do one or more things *after* it scrapes/loads the data.

  * **Single User Processing:**
      * You *must* give it a `USERNAME`.
      * It'll scrape/update that user's data first.
      * Then, you can tell it to:
          * Generate statistics (`--generate-stats`).
          * Run AI analysis (`--run-analysis`).
          * Do both (`--generate-stats --run-analysis`).
          * Or just scrape and export JSON/CSV (`--export-json-only`).
  * **Comparing Two Users:**
      * Use the `--compare-user USER1 USER2` flag.
      * You **don't** need a `USERNAME` argument at the very beginning of the command for this one (it gets the names from the flag).
      * It loads the *existing* statistical JSON data for both users from your output directory. It *won't* generate new stats right then unless those data files are missing.
      * Creates a side-by-side comparison report.
      * Note: Filters (`--start-date`, etc.) are ignored in this mode because it's comparing the full dataset already saved in the stats files.
  * **Monitoring a User:**
      * Use the `--monitor USERNAME` flag.
      * You *do* need to give the `USERNAME` as an argument right after `--monitor`.
      * It just hangs out in a continuous loop, checking for new activity for that user every so often.
      * When new stuff pops up, it scrapes it, updates the main JSON file, and exports the updated CSVs.
      * By default, it doesn't automatically run stats or AI analysis every time it finds new activity (could add that later maybe).
      * It uses the `monitor_interval_seconds` setting from your config or whatever you set with the `--monitor-interval` flag.
  * **Generating AI Prompts:**
      * Use the `--generate-prompt` flag.
      * You **don't** need a `USERNAME` argument for this.
      * This starts a little interactive chat with the Gemini API right in your terminal to help you brainstorm and refine a system prompt for AI analysis.
      * It saves the prompt you end up with to a file in your `default_prompt_dir`.
  * **Exporting JSON Data Only:**
      * Use the `--export-json-only USERNAME` flag.
      * You *do* need a `USERNAME` argument at the start for this.
      * It just does the scraping/updating part, saves the `username.json` file, and then exports the filtered data into `username-posts.csv` and `username-comments.csv`.
      * It stops there, no stats or AI reports. Handy if you just want to keep your data files fresh.

### Examples üöÄ

Here are some common ways you might use RedStalk:

1.  **Scrape/Update data for `SomeRedditor`, get their stats report, and run AI analysis using the default prompt:**

    ```bash
    python redstalk.py SomeRedditor --generate-stats --run-analysis
    ```

2.  **Generate Stats for `UserX`, but *only* for activity between specific dates:**

    ```bash
    python redstalk.py UserX --generate-stats --start-date 2023-01-01 --end-date 2023-12-31
    ```

    *(Quick note: Dates should be in YYYY-MM-DD format, and it uses UTC time.)*

3.  **Generate Stats for `UserY`, focusing *only* on stuff in `r/python` and `r/datascience`:**

    ```bash
    python redstalk.py UserY --generate-stats --focus-subreddit python datascience
    ```

    *(Just give the subreddit names without the `r/`.)*

4.  **Generate Stats for `UserZ`, *excluding* activity from `r/politics` and `r/news`:**

    ```bash
    python redstalk.py UserZ --generate-stats --ignore-subreddit politics news
    ```

5.  **Mix filters for `AnalystBob`: analyze recent activity in specific investment subreddits while ignoring news:**

    ```bash
    python redstalk.py AnalystBob --generate-stats --start-date 2024-01-01 --focus-subreddit investing stocks wallstreetbets --ignore-subreddit news worldnews
    ```

6.  **Run AI Analysis on `CreativeWriter`'s *filtered* data (using the default prompt and `raw` mode):**

    ```bash
    python redstalk.py CreativeWriter --run-analysis --analysis-mode raw
    ```

7.  **Run AI Analysis on `TechGuru`'s *filtered* data using a custom prompt file and `mapped` mode, and fetch those external post titles for extra context:**

    ```bash
    python redstalk.py TechGuru --run-analysis --prompt-file prompts/tech_analysis_prompt.txt --analysis-mode mapped --fetch-external-context
    ```

8.  **Generate Stats and AI Analysis for `HelpfulUser`, *plus* get the AI to summarize the stats report:**

    ```bash
    python redstalk.py HelpfulUser --generate-stats --run-analysis --summarize-stats
    ```

    *(Remember, `--summarize-stats` only works if you're also using `--generate-stats` and `--run-analysis`.)*

9.  **Compare the statistical reports of `UserA` and `UserB`:**

    ```bash
    python redstalk.py --compare-user UserA UserB
    ```

    *(This will look for the existing `_stats_data.json` or Markdown stats files for both users in your output folder.)*

10. **Monitor `NewsJunkie` for new activity, checking every 10 minutes (that's 600 seconds):**

    ```bash
    python redstalk.py NewsJunkie --monitor --monitor-interval 600
    ```

11. **Want to create a new AI system prompt? Start the interactive prompt generator:**

    ```bash
    python redstalk.py --generate-prompt
    ```

12. **Just scrape and update the JSON and CSV files for `DataHoarder`, without making any reports:**

    ```bash
    python redstalk.py DataHoarder --export-json-only
    ```

13. **Force a *complete* re-scrape of *all* activity for `OldProfile`, ignoring any old JSON data, and *then* generate stats:**

    ```bash
    python redstalk.py OldProfile --generate-stats --force-scrape
    ```

### Key Options Detailed

  * `USERNAME`: The Reddit username you want to work with (you need this for single-user stuff and monitoring).
  * `--generate-stats`: Use this flag to tell it to do the statistical analysis and make the report.
  * `--run-analysis`: Use this flag to trigger the AI-powered text analysis.
  * `--compare-user USER1 USER2`: This is an *exclusive* action flag for comparing two users' stats. You give it the two usernames right after the flag.
  * `--monitor USERNAME`: This is another *exclusive* action flag. Use it to start monitoring a user for new activity. You need to give the username right after this flag.
  * `--generate-prompt`: An *exclusive* action flag to kick off the interactive AI prompt generator.
  * `--export-json-only`: An *exclusive* action flag to *just* scrape/update the data files (JSON and CSV) and then stop. Needs a USERNAME argument.
  * `--reset-config`: An *exclusive* action flag to reset your `config.json` file back to the script's defaults.
  * `--output-dir DIR`: Want to save everything somewhere else? Use this to tell it a different base directory. This overrides the `default_output_dir` setting.
  * `--log-level LEVEL`: Control how chatty the logging output is. Options are DEBUG, INFO, WARNING, ERROR, CRITICAL. Default is INFO.
  * `--start-date YYYY-MM-DD`: Filter activity to *only* include stuff created on or after this date (in UTC time).
  * `--end-date YYYY-MM-DD`: Filter activity to *only* include stuff created on or before this date (in UTC time).
  * `--focus-subreddit SUB1 SUB2 ...`: Give it a list of subreddit names (separated by spaces). It will *only* include activity within these subreddits.
  * `--ignore-subreddit SUB1 SUB2 ...`: Give it a list of subreddit names (separated by spaces). It will *exclude* activity within these subreddits.
  * `--force-scrape`: Ignore any `username.json` file that exists and download *all* available posts and comments from the Reddit API again. Good if you think your data file is messed up or you just need the absolute latest possible data. Use with caution though, it makes lots of requests.
  * `--scrape-comments-only`: When scraping, just grab comments and skip fetching posts.
  * `--prompt-file FILE`: Tell it where to find a custom system prompt file for AI analysis. This overrides the `default_prompt_file` setting.
  * `--api-key KEY`: You can put your Google Gemini API key right here on the command line. This takes priority over everything else but is the *least* secure way.
  * `--analysis-mode MODE`: Choose how the AI processes the data: `mapped` (groups comments under posts for context) or `raw` (just a simple list of activities chronologically). Default is `mapped`.
  * `--fetch-external-context`: If you're using `mapped` analysis mode, this tries to get the titles for any posts that link to external websites. Helps the AI understand what those links were about. Uses extra API calls.
  * `--summarize-stats`: If you're also using `--generate-stats`, this tells the AI to generate a summary of the statistical report.
  * `--top-words N`: How many top words and n-grams you want in the statistical report's frequency lists. Default is 50.
  * `--top-items N`: How many top/bottom posts/comments (by upvotes/engagement) to list in the statistical report. Default is 10.
  * `--stats-output-json FILE`: Save the calculated statistics data (before it gets formatted into Markdown) as a JSON file. Useful if you want to grab that data programmatically. By default, it saves to `username_stats_data.json` in the user's directory.

## Statistical Analysis (`stats` module) üìä

The `stats` package is a key part for doing that quantitative analysis. It takes the filtered user data (from the CSVs or JSON) and calculates a ton of metrics using `calculations.py`. Then `reporting.py` turns all those results into a detailed Markdown report that's easy to read. The comparison report (`comparison.py`) uses these same stats too.

### Available Metrics

The stats report covers quite a few things:

  *   **Basic Info:** Username, the time span of their activity, when their account was created, karma breakdown (link/comment), total posts/comments.
  *   **Subreddit Engagement:**
      *   How many unique subreddits they participated in.
      *   A list of their top subreddits based on activity count.
      *   Diversity measures (like Shannon index) to see how spread out their activity is across different subreddits.
      *   A breakdown of activity counts and percentages per subreddit.
  *   **Content Analysis:**
      *   Total word count in their posts and comments.
      *   Lexical diversity (Type-Token Ratio) ‚Äì kinda shows how rich their vocabulary might be.
      *   Breakdown of whether their posts were self-text or link posts.
  *   **Engagement Metrics:**
      *   Average upvotes per post/comment.
      *   Total/Average awards received.
      *   Average comments their posts got.
      *   Lists of their best and worst performing posts and comments (by upvotes).
  *   **Metadata Analysis:**
      *   Looking at how they used flair (text, CSS class).
      *   How often they edited their posts/comments.
      *   An estimated percentage of content that seems to have been deleted.
      *   How often they crossposted stuff.
  *   **Temporal Patterns:**
      *   When they're active ‚Äì broken down by hour of the day, day of the week, month, and year.
      *   An analysis of how "bursty" their posting is (like, do they post a lot in short periods?).
  *   **Simple NLP:**
      *   The most frequent words and n-grams (those sequences of words) in their posts and comments.
      *   VADER sentiment analysis: Gives you a ratio of positive, negative, and neutral sentiment, plus sentiment scores over time (the sentiment arc).
      *   How often they mention other users (`u/`) or subreddits (`r/`) ‚Äì found using some regex magic.

## AI-Powered Analysis (`analysis` module) ü§ñ

The `analysis` module is where the Google Gemini API comes in for the qualitative, narrative side of things based on the user's text. This lets you do cool stuff like generate character profiles, find recurring themes, analyze writing style, or whatever else you tell the AI to do in your system prompt.

Here's kinda how it works:

1.  It loads the filtered post and comment data you have.
2.  Formats the data based on the `--analysis-mode` you chose.
3.  Uses the Gemini API's token counter to figure out how to break the data into chunks that fit within the model's context window (based on your `default_chunk_size`).
4.  Sends each chunk to the Gemini model along with your system prompt.
5.  Collects the AI's responses for each chunk.
6.  Puts the responses together (if it makes sense for the prompt) and saves the final analysis report.

### Analysis Modes (`mapped` vs `raw`)

  *   **`mapped` Mode (Default):** This mode tries to give the AI more context for conversations. It groups comments right under the parent posts they belong to. The data sent to the AI looks something like this:
    ```
    ## Post Title (or the External URL)
    Post Body
    > Comment 1
    >> Reply to Comment 1
    > Comment 2
    ...
    ```
    This is usually better if your prompt asks the AI to analyze interactions or discussions. Use `--fetch-external-context` to help the AI out with titles for linked external content.
  *   **`raw` Mode:** This is simpler. It just gives the AI a chronological list of the user's posts and comments, no fancy grouping. Just a sequence like:
    ```
    Post 1 Title
    Post 1 Body
    Comment 1 Text
    Post 2 Title
    ...
    ```
    This might be better if your prompt is focused purely on writing style, overall themes no matter the context, or if the post-comment relationship isn't important for your analysis.

### Prompt Engineering

Okay, real talk: how good the AI analysis is *really* depends on how good your system prompt is. A great prompt clearly tells the AI what role to play, what data it's looking at, what task to do, what format you want the output in, and any rules it needs to follow.

The `--generate-prompt` flag starts this cool interactive assistant that chats with the Gemini API itself to help you write and tweak a system prompt. It walks you through defining the AI's persona, the data context, what you're trying to figure out, and how you want the result formatted. Highly recommend using this if you're new to writing system prompts or just want to mess around with different analysis ideas.

## Output Files üìÇ

RedStalk keeps your output files organized. They all go into the `default_output_dir` (or whatever directory you specified with `--output-dir`). It usually creates a subfolder for each user you analyze.

  *   `/[output_dir]/[username]/[username].json`: This is the main file with all the raw scraped JSON data (posts and comments).
  *   `/[output_dir]/[username]/[username]-posts.csv`: A CSV file with the filtered post data.
  *   `/[output_dir]/[username]/[username]-comments.csv`: A CSV file with the filtered comment data.
  *   `/[output_dir]/[username]/[username]_stats_YYYYMMDD_HHMMSS.md`: The Markdown report for the statistical analysis of a single user. The timestamp helps if you run reports multiple times.
  *   `/[output_dir]/[username]/[username]_stats_data.json` (Optional): If you used `--stats-output-json`, this is the JSON file with the raw calculated stats.
  *   `/[output_dir]/[username]/[username]_charc_[mode]_YYYYMMDD_HHMMSS.md`: The Markdown report with the AI analysis results. `[mode]` will say `mapped` or `raw` depending on what you chose.
  *   `/[output_dir]/comparison_[USER1]_vs_[USER2]_YYYYMMDD_HHMMSS.md`: The Markdown report comparing the stats of two users. This one goes right into the base output directory, not a user subfolder.
  *   `/[output_dir]/prompts/`: This is the default place where custom system prompt files generated by `--generate-prompt` are saved.

## How It Works (High Level Architecture) üèóÔ∏è

Here's a quick look at what's happening under the hood:

1.  **Starting Up (`redstalk.py`):** Reads the command-line stuff you typed, loads settings from `config.json`, and sets up the logging (that's where the pretty colors come from!).
2.  **Figuring Out What To Do (`redstalk.py`):** Based on your flags, it decides the main action(s) ‚Äì scrape, stats, analysis, compare users, monitor, generate prompt, reset config, or just export.
3.  **Getting the Data (`reddit_utils.py`):**
    *   Handles talking to the Reddit API using the `requests` library and the `user_agent` you set up.
    *   Takes a username and grabs their posts and comments chronologically.
    *   It does that smart incremental scraping thing: checks your existing `.json` file for the latest activity timestamp and *only* fetches newer entries from Reddit. Saves API calls and speeds up future runs!
    *   Merges the new data with your old data and saves the updated file as `username.json`.
    *   Tries to handle Reddit's API rate limits nicely by waiting and retrying if needed.
4.  **Getting Data Ready (`data_utils.py`):**
    *   Loads *all* the data from `username.json`.
    *   Applies your filters (date range, focus/ignore subreddits).
    *   Separates the filtered data into lists for posts and comments.
    *   Saves these filtered lists into `username-posts.csv` and `username-comments.csv`.
5.  **Doing the Stats (`stats/` package):**
    *   Comes into play when you use `--generate-stats` or `--compare-user`.
    *   Reads the filtered data (sometimes from CSVs because it's faster for some calculations).
    *   `calculations.py`: This is where all the functions live that compute those statistical metrics from the data. It uses optional dependencies like `vaderSentiment`, and `pandas`.
    *   `reporting.py`: Takes the results from `calculations.py` and formats them into a structured Markdown report for a single user.
    *   `comparison.py`: Loads stats for two users (either from those pre-saved JSON files or by running calculations if needed), figures out things like overlap, and makes that side-by-side Markdown comparison report.
6.  **Running the AI Analysis (`analysis.py`, `ai_utils.py`):**
    *   Gets activated when you use `--run-analysis` or `--summarize-stats`.
    *   Reads the filtered data (from CSVs).
    *   `ai_utils.py`: Contains the core stuff for talking to the Google Gemini API.
        *   `chunk_items`: Breaks the data into chunks that fit the target token size (`default_chunk_size`) and the model's limits. Uses the API's token counter feature.
        *   `perform_ai_analysis`: Sends a data chunk and the system prompt to the chosen Gemini model and gets the response back. Handles API calls and potential issues.
    *   `analysis.py`: Manages the whole AI analysis flow. Reads your prompt file, organizes the data based on your analysis mode (`mapped` or `raw`), tells `ai_utils` to process the chunks, and puts the results together into the final Markdown report. Also handles the `--fetch-external-context` and `--summarize-stats` options.
7.  **Monitoring Loop (`monitoring.py`):**
    *   Runs when you use `--monitor`.
    *   Just sits in a loop, calling the scraping function (`reddit_utils.py`) periodically to check for new activity at your specified interval.
    *   If it finds new stuff, it triggers the data saving and CSV export steps (`data_utils.py`).
8.  **Prompt Generation (`ai_utils.py`):**
    *   Starts up when you use `--generate-prompt`.
    *   Sets up an interactive chat session using the Gemini API to guide you through writing and improving a system prompt.
    *   Saves your final prompt text to a file.

## Dependencies üîó

What does RedStalk rely on?

  *   **Core Stuff:**
      *   `requests`: Gotta have this for talking to the Reddit API over HTTP.
      *   `rich`: Makes the terminal output look much better with colors and progress bars. Nice for user experience!
  *   **AI Features (Optional):**
      *   `google-generativeai`: The official Python library to talk to the Google Gemini API. Essential for any AI features.
      *   `tqdm`: Shows those cool progress bars when it's counting tokens for AI analysis, so you know it's working on big datasets.
  *   **Statistics Features (Optional):**
      *   `vaderSentiment`: A sentiment analysis tool that's designed for social media text. Used for sentiment metrics in the stats report.
      *   `pandas`: A really strong library for handling data. Used in some stats calculations (especially with CSVs) for efficient data manipulation and analysis.

## Contributing ü§ó

Hey, we'd totally love contributions to RedStalk! Whether you find a bug, have a cool feature idea, want to make the documentation better, submit code with a pull request, or (honestly, *please*) help make the current code vibe a bit less like... well, AI-coded bullshit ‚Äì your help is welcome!

1.  **Fork the repo:** Head over to GitHub and fork the `redstalk` repository.
2.  **Clone your fork:** Get a copy of *your* forked repo onto your computer.
3.  **Make a new branch:** Create a fresh branch for your new feature or bugfix (like `git checkout -b my-awesome-feature`).
4.  **Make your changes:** Code away! And test your changes thoroughly, please. üôè
5.  **Write docs:** Update the README.md or add specific documentation explaining your changes.
6.  **Commit your changes:** Save your changes with commit messages that are clear and to the point.
7.  **Push to your fork:** Upload your changes to your fork on GitHub (`git push origin my-awesome-feature`).
8.  **Open a Pull Request:** Go back to GitHub and open a pull request from your fork to the main `redstalk` repo. Describe what you did and why!

Just try to make sure your code fits the project's style (if you can even call it that üòÇ) and that everything passes any tests.

## License (COPYLEFT FTW) üìú

This project is under the GNU General Public License v3.0 (GPL v3).

The GPL v3 is a free license, and it's "copyleft." What does that mean? The main points are:

*   **Run it:** You can use the program for *any* reason you want.
*   **Study and Change it:** You're free to look at how it works and change it. Access to the source code is a must for this.
*   **Share Copies:** You can give copies of the program to other people.
*   **Share Modified Versions:** You can distribute copies of any versions you changed to others.

Here's the copyleft part: If you share the software, or anything you made *based* on it, you have to share it under the *exact same license* (GPL v3). You also have to include the source code and make sure the people you give it to have the same four freedoms you did.

Check out the `LICENSE` file in the main project folder for the full legal text.

## Disclaimer and Ethical Considerations ‚ö†Ô∏è

  *   **Reddit API Rules:** Look, this tool uses the public Reddit API. How *you* use RedStalk **absolutely must follow** the [Reddit API Terms of Service](https://www.redditinc.com/policies/developer-terms). Seriously, pay attention to the User-Agent stuff and the rate limits.
  *   **User-Agent:** Gonna say it again because it's *that* important: setting a unique and descriptive `user_agent` with your Reddit username in it is **MANDATORY** per Reddit's terms. If you don't, things can break.
  *   **Ethical Use:** This tool grabs data that's out there publicly. But analyzing people and maybe even profiling them based on their online activity brings up some big ethical questions.
      *   **Respect Privacy:** Yeah, the data is public, but think about the privacy side of analyzing it. DO NOT use this tool to doxx people, harass anyone, stalk, or do anything else malicious. Just don't.
      *   **Be Transparent:** If you're using this for research or sharing findings publicly, be clear about how you did it and where the data came from.
      *   **Consent:** If your analysis is about specific individuals and you're going to share findings in a way that people could figure out who they are, maybe think about whether you need or should get their consent.
  *   **AI Accuracy:** Remember, AI models can sometimes just make stuff up or get things wrong. The AI analysis is only as good as the data you give it, the model you use, and the prompt you write. Don't take AI analysis as the absolute truth for important decisions.
  *   **No Liability:** The people who made RedStalk (which is currently just me, myself, and I, plus hopefully cool future open source folks) are NOT responsible or liable for anyone misusing this tool, anything bad that happens because you used it, or you breaking Reddit's rules or ethical guidelines. You are 100% responsible for what you do with RedStalk.

**Seriously, use this tool responsibly and ethically.** Got it? Good. üëç

## Troubleshooting ü§î

Hit a snag? Here are a few common issues and what to check:

  *   **`ModuleNotFoundError`:** Make sure you've activated your virtual environment (`source venv/bin/activate` or `venv\Scripts\activate`) and installed *all* the dependencies (`pip install -r requirements.txt`).
  *   **Reddit API Rate Limits:** If you keep seeing errors about hitting rate limits, it means you're sending too many requests too fast.
      *   Double-check that your `user_agent` is set up correctly in `config.json`.
      *   The incremental scraping is designed to avoid this, but `--force-scrape` will send a lot of requests. Don't use it more than you have to.
      *   Keep in mind, Reddit's API rules and rate limits can change. Check their official docs if you're running into constant problems.
  *   **Gemini API Errors:**
      *   Is your API key set right? Check your environment variable, `config.json`, or the `--api-key` flag (in that order of priority).
      *   Did you enable the Gemini API in your Google Cloud project or Google AI Studio?
      *   Is the model name in `config.json` (`default_model_name`) correct and available to you?
      *   If you're getting token limit errors even with chunking, you might need to lower the `default_chunk_size` or try a Gemini model with a bigger context window.
  *   **Filtering Not Working:** Make sure your dates are in the right format (`YYYY-MM-DD`) and your subreddit names don't have the `r/` prefix when you use the filter flags.
  *   **Config Not Applying:** Remember how settings have priority: Command Line Flags > Environment Variables > `config.json`. Is a higher-priority setting accidentally overriding what you put in `config.json`?

If you're still stuck after checking these things, please open an issue on the GitHub repo. Include details about the problem, your operating system, Python version, RedStalk version, and the exact command you were running.

## Future Enhancements ‚ú®üîÆ

Here are some ideas for stuff that could be added to RedStalk down the road:

~~1.  **Sentiment Arc:** (Simple NLP) - Show how sentiment changes over time.~~
~~2.  **Question Asking Ratio:** (Simple NLP) - Estimate how often a user asks questions.~~
~~3.  **Mention Frequency:** (Simple NLP) - List the users/subreddits they mention most.~~

4.  **AI Subreddit Categorization:** (Potential AI) - Use AI to group subreddits into broader categories.

5.  **AI Topic Modeling / Keyword Extraction:** (Potential AI) - Get the AI to find key topics and keywords in their text.

6.  **AI Behavioral Trait Identification:** (Potential AI) - Ask the AI to identify traits based on text (like "helpful").

7.  **Reply Depth Analysis:** (Deeper Interaction) - Look at how deep their comments go in reply chains.

8.  **Content Originality Estimate:** (Deeper Interaction) - Compare engagement for self posts vs. link posts to get an idea.

9.  **Interactive Console Stats:** (Utility) - Show the stats right in the terminal using `rich` instead of saving to a file.

10. **Sentiment Trajectory Analysis:** (Advanced) - More detailed look at sentiment trends, maybe per-subreddit.

11. **Linguistic Style Markers & Consistency:** (Advanced) - Analyze things like sentence length, punctuation, caps, etc., and see how consistent their style is, maybe across subreddits.

12. **Interaction Role Tendencies:** (Advanced AI Assist) - Use AI to estimate if they tend to be more of a question-asker, answer-giver, etc., in different subs.

13. **Subreddit Persona Comparison:** (Advanced AI Assist) - Run AI analysis *separately* on text from different top subs and compare the AI's character profiles.

14. **"Rabbit Hole" / Topic Burst Detector:** (Advanced) - Find times when they have a concentrated burst of activity in just one subreddit over a short period.

Got any other cool ideas? Open an issue on the GitHub repo and let us know! üëá
