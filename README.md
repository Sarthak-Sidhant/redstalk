# RedStalk v1.9.0 üïµÔ∏è‚Äç‚ôÄÔ∏è - Advanced Reddit User Activity Analysis
<sub> this whole readme.md was generated by gemini-2.5-pro, combined with around 60% to 70% of the code in here. please don't blame us for this. this was supposed to be a thursday project </sub>

**RedStalk** is a sophisticated command-line utility designed for comprehensive analysis of public Reddit user activity. By intelligently scraping, filtering, statistically analyzing, and leveraging advanced AI capabilities, RedStalk provides deep insights into user behavior patterns, interests, communication style, sentiment, and more, all derived from their publicly available posts and comments.

Whether you are a researcher studying online communities, a data analyst examining user trends, or simply curious about the public footprint of a specific user, RedStalk offers a powerful toolkit to explore and understand Reddit interactions.

<sub> while searching for redstalk on google and bing, i came across</sub> [the og redstalk](https://www.redstalk.net/)<sub> and i absolutely fw their art and design. check them out, sorry for using the name - it was too late when i realized</sub>

[![Python Version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-AGPLv3-orange.svg)](https://opensource.org/license/agpl-v3)
[![GitHub Repository](https://img.shields.io/badge/GitHub-redstalk-brightgreen)](https://github.com/sarthak-sidhant/redstalk) 
## Table of Contents

1.  [Overview](#overview)
2.  [Features](#features)
3.  [Installation](#installation)
    * [Prerequisites](#prerequisites)
    * [Cloning the Repository](#cloning-the-repository)
    * [Virtual Environment (Recommended)](#virtual-environment-recommended)
    * [Installing Dependencies](#installing-dependencies)
    * [Downloading NLTK Data (Optional)](#downloading-nltk-data-optional---for-stats)
4.  [Configuration](#configuration)
    * [Key `config.json` Settings](#key-configjson-settings)
    * [Setting your Google Gemini API Key](#setting-your-google-gemini-api-key)
    * [Setting your Reddit User-Agent](#setting-your-reddit-user-agent)
    * [Resetting Configuration](#resetting-configuration)
5.  [Usage](#usage)
    * [Basic Syntax](#basic-syntax)
    * [Getting Help](#getting-help)
    * [Core Actions Explained](#core-actions-explained)
        * [Single User Processing](#single-user-processing)
        * [Comparing Two Users](#comparing-two-users)
        * [Monitoring a User](#monitoring-a-user)
        * [Generating AI Prompts](#generating-ai-prompts)
        * [Exporting JSON Data Only](#exporting-json-data-only)
    * [Examples](#examples)
    * [Key Options Detailed](#key-options-detailed)
6.  [Statistical Analysis (`stats` module)](#statistical-analysis-stats-module)
    * [Available Metrics](#available-metrics)
7.  [AI-Powered Analysis (`analysis` module)](#ai-powered-analysis-analysis-module)
    * [Analysis Modes (`mapped` vs `raw`)](#analysis-modes-mapped-vs-raw)
    * [Prompt Engineering](#prompt-engineering)
8.  [Output Files](#output-files)
9.  [How It Works (High Level Architecture)](#how-it-works-high-level-architecture)
10. [Dependencies](#dependencies)
11. [Contributing](#contributing)
12. [License](#license)
13. [Disclaimer and Ethical Considerations](#disclaimer-and-ethical-considerations)
14. [Troubleshooting](#troubleshooting)
15. [Future Enhancements](#future-enhancements)

## Overview

RedStalk provides an end-to-end workflow for analyzing Reddit user profiles. Starting from fetching raw data using the Reddit API, it proceeds to filter, process, and analyze the information through both traditional statistical methods and advanced natural language processing powered by Google's Gemini AI. The goal is to transform a user's public activity stream into structured insights and narrative summaries.

The tool is designed with efficiency in mind, utilizing incremental scraping to update existing datasets and offering flexible filtering options to narrow down the scope of analysis. All results are saved into organized, human-readable file formats (Markdown, CSV, JSON) for easy review and further processing.

## Features

* **Efficient Data Scraping:** Fetches user posts and comments from the Reddit API. It intelligently updates existing data files by only requesting activity newer than the most recent entry saved locally, significantly speeding up subsequent runs. Supports fetching comments only (`--scrape-comments-only`) and forcing a full re-scrape (`--force-scrape`).
* **Granular Data Filtering:** Allows precise control over which data is included in the analysis:
    * **Date Range:** Filter activity between specific start and end dates (`--start-date`, `--end-date`).
    * **Subreddit Inclusion:** Focus analysis *only* on posts/comments within specified subreddits (`--focus-subreddit`).
    * **Subreddit Exclusion:** Ignore posts/comments from specified subreddits (`--ignore-subreddit`). Filters can be combined.
* **Multiple Output Formats:**
    * **Raw Data:** Saves all scraped posts and comments as `.json` files (one per user).
    * **Filtered Data:** Exports the filtered dataset (after applying date/subreddit constraints) to separate `.csv` files for posts and comments, making it easy to import into spreadsheets or other analysis tools.
    * **Statistical Reports:** Generates detailed, structured reports in `.md` (Markdown) format covering numerous quantitative metrics.
    * **AI Analysis Reports:** Produces narrative summaries, character profiles, or other insights generated by the AI, also in `.md` format.
    * **Comparison Reports:** Creates side-by-side statistical comparisons of two users in `.md` format.
    * **Optional JSON Stats Export:** Allows saving the raw statistical data calculated by the `stats` module into a `.json` file for programmatic access.
* **Comprehensive Statistical Analysis:** (Requires `stats` dependencies) Calculates a wide array of metrics including:
    * Activity Timelines (Temporal Patterns)
    * Subreddit Engagement (Diversity, Frequency, Top Subs)
    * Content Characteristics (Word Counts, Lexical Diversity, Question Ratio, Post/Comment Type Breakdown)
    * Engagement Scores (Upvotes, Awards, Comments per Post)
    * Metadata Insights (Flair Usage, Editing Frequency, Deletion Estimates)
    * Simple NLP Metrics (Word/N-gram Frequencies, Sentiment Analysis using VADER)
* **AI-Powered Deep Analysis:** (Requires Google Gemini API Key) Utilizes the Gemini API to perform qualitative analysis based on the text content of user activity:
    * **Custom Prompts:** Analysis is driven by a user-defined system prompt, allowing you to tailor the AI's task (e.g., create a personality profile, summarize opinions on a topic, analyze writing style). Prompts are loaded from files (`--prompt-file`).
    * **Analysis Modes:** Choose between `mapped` mode (grouping comments under their parent posts for context) and `raw` mode (processing posts and comments chronologically).
    * **External Context Fetching:** In `mapped` mode, optionally fetches titles for external URLs posted by the user to provide the AI with more context (`--fetch-external-context`).
    * **Token Handling:** Efficiently processes large volumes of text by intelligently chunking the data to fit within the AI model's token limits.
    * **AI Stats Summary:** Option to have the AI generate a summary of the statistical report (`--summarize-stats`).
* **User Comparison:** Provides a dedicated mode (`--compare-user`) to generate a single Markdown report comparing the statistical profiles of two specified users side-by-side.
* **Real-time Monitoring:** A background mode (`--monitor`) that periodically checks a target user for new activity. If new posts or comments are found, it automatically scrapes the new data, updates the JSON file, and exports the updated CSVs. (Optionally triggers analysis on update - configurable).
* **Interactive Prompt Engineering:** Includes a guided command-line assistant (`--generate-prompt`) to help users craft effective and specific system prompts for the AI analysis module.
* **Flexible Configuration:** Default settings can be easily managed via a `config.json` file or overridden using command-line flags. Supports API key management via environment variable for security.
* **Enhanced Logging:** Uses colored console output (`rich` library) to provide clear feedback on the tool's progress, status, and any issues encountered.

## Installation

Follow these steps to get RedStalk up and running on your system.

### Prerequisites

* **Python 3.8 or higher:** Download and install from [python.org](https://www.python.org/downloads/).
* **`pip`:** Python's package installer (usually included with modern Python installations).
* **`git`:** Version control system for cloning the repository. Install from [git-scm.com](https://git-scm.com/downloads).

### Cloning the Repository

Open your terminal or command prompt and run:

```bash
git clone https://github.com/sarthak-sidhant/redstalk.git
cd redstalk
````

### Virtual Environment (Recommended)

It's highly recommended to use a virtual environment to isolate RedStalk's dependencies from your system's Python packages.

```bash
# Create a virtual environment named 'venv'
python -m venv venv

# Activate the virtual environment:

# On Windows:
venv\Scripts\activate

# On macOS/Linux:
source venv/bin/activate
```

You will see `(venv)` at the beginning of your terminal prompt when the environment is active.

### Installing Dependencies

Create a file named `requirements.txt` in the root directory of the cloned repository (the same directory as `redstalk.py`) and paste the following:

```txt
# Core Dependencies for basic scraping and file handling
requests>=2.25.0
rich>=10.0.0 # For colored logging and progress bars

# Optional Dependencies - Install based on the features you plan to use

# Required for all AI Features (--run-analysis, --summarize-stats, --generate-prompt)
google-generativeai>=0.3.0

# Required for Statistical Analysis (--generate-stats, --compare-user)
vaderSentiment>=3.3.0       # For VADER-based sentiment analysis
pandas>=1.2.0               # Used in some calculations for data handling

# Optional, but recommended for better progress bars in AI analysis
tqdm>=4.60.0 # Will be automatically installed if google-generativeai pulls it, but listed for clarity
```

Now, install the dependencies using pip:

```bash
pip install -r requirements.txt
```

## Configuration

RedStalk uses a `config.json` file for default settings. A basic `config.json` might be included with the repository. If not, create one in the project root directory.

**Example `config.json`:**

```json
{
    "default_output_dir": "data",
    "default_prompt_file": "default_prompt.txt",
    "default_prompt_dir": "prompts",
    "default_chunk_size": 1000000,
    "api_key": "put in your own",
    "default_model_name": "gemini-2.0-flash",
    "monitor_interval_seconds": 300,
    "user_agent": "Python:RedStalkScript:v1.9 (by /u/YourRedditUsername)"
}
```

### Key `config.json` Settings

  * **`default_output_dir`**: Specifies the primary directory where all user data files (`.json`, `.csv`, reports) will be saved. User-specific files will typically be saved in subdirectories named after the username within this directory. Default: `"data"`.
  * **`default_prompt_file`**: The default system prompt file (located within `default_prompt_dir`) to use for AI analysis when `--run-analysis` is specified without `--prompt-file`. Default: `"default_prompt.txt"`.
  * **`default_prompt_dir`**: The directory where AI prompt files are stored and where the interactive prompt generator saves new prompts. Default: `"prompts"`.
  * **`default_chunk_size`**: This is the target token size for data chunks sent to the Gemini API during AI analysis. The actual size may vary slightly. A smaller chunk size might be necessary for models with lower context windows or for more granular analysis, while larger sizes can process more text in one go. Note: The provided example `1000000` is suitable for models like `gemini-1.5-flash` which have a large context window. Adjust based on your chosen model and analysis needs. Default: `1000000`.
  * **`api_key`**: **Your Google Gemini API Key.** While you can set this here, it's **more secure** to use the `GOOGLE_API_KEY` environment variable.
  * **`default_model_name`**: The name of the Google Gemini model you wish to use for AI analysis. Recommended models with large context windows include `gemini-1.5-flash` or `gemini-1.5-pro`. Use a model that supports the desired token chunk size. Default: `"gemini-1.5-flash"`.
  * **`monitor_interval_seconds`**: The default time interval (in seconds) between checks when using the `--monitor` mode. Default: `300` (5 minutes).
  * **`user_agent`**: **CRITICAL\!** **You MUST set a unique and descriptive User-Agent string here, including your Reddit username.** This is a mandatory requirement of the Reddit API Terms of Service. Replace `/u/YourRedditUsername` with your actual Reddit username. Example: `"MyRedditAnalysisBot/1.0 (by /u/YourUsername)"`. Failure to set a proper User-Agent can lead to your requests being rate-limited or blocked by Reddit.

### Setting your Google Gemini API Key

Your API key is required for any AI-related features (`--run-analysis`, `--summarize-stats`, `--generate-prompt`). **For security reasons, using an environment variable is the preferred method.**

1.  **Get your API Key:** Obtain your key from [Google AI Studio](https://aistudio.google.com/app/apikey) or the Google Cloud Console.
2.  **Set Environment Variable (Recommended):** Set an environment variable named `GOOGLE_API_KEY` to your API key.
      * **On Windows:**
        ```cmd
        set GOOGLE_API_KEY=YOUR_API_KEY_HERE
        ```
        (For persistent setting, use System Properties -\> Environment Variables)
      * **On macOS/Linux:**
        ```bash
        export GOOGLE_API_KEY='YOUR_API_KEY_HERE'
        ```
        (Add this line to your shell's profile file, e.g., `~/.bashrc`, `~/.zshrc`, for persistent setting)
3.  **Set in `config.json` (Less Secure):** If you cannot use an environment variable, you can paste your key into the `api_key` field in `config.json`.
4.  **Set via Command Line (Least Secure, Highest Priority):** You can provide the key directly using the `--api-key` flag. This is not recommended for regular use but can be helpful for testing.

RedStalk checks for the API key in this order: Command Line Flag \> Environment Variable \> `config.json`.

### Setting your Reddit User-Agent

As mentioned above, setting a proper User-Agent is **mandatory** and **critical** for complying with Reddit's API terms. Edit the `config.json` and replace `"by /u/YourRedditUsername"` with your actual Reddit username. This helps Reddit identify your script and contact you if needed.

### Resetting Configuration

If you mess up your `config.json` or want to revert to the default settings provided by the script, you can use the `--reset-config` flag:

```bash
python redstalk.py --reset-config
```

This will overwrite your current `config.json` with the default values hardcoded in the script.

## Usage

RedStalk is a command-line application. You execute it using `python redstalk.py` followed by the target username (for single-user actions) and various action flags and options.

### Basic Syntax

```bash
python redstalk.py [USERNAME] [ACTIONS...] [OPTIONS...]
```

Or, for actions that don't target a single user initially (like comparisons or prompt generation):

```bash
python redstalk.py [EXCLUSIVE_ACTION] [OPTIONS...]
```

### Getting Help

To view a list of all commands, options, and a brief description of each:

```bash
python redstalk.py --help
# or
python redstalk.py -h
```

### Core Actions Explained

You typically specify one or more actions for RedStalk to perform after data scraping/loading.

  * **Single User Processing:**
      * Requires a `USERNAME` argument.
      * Involves scraping/updating the user's data first.
      * Then, you can choose to:
          * Generate statistics (`--generate-stats`).
          * Run AI analysis (`--run-analysis`).
          * Both (`--generate-stats --run-analysis`).
          * Only export JSON/CSV after scraping (`--export-json-only`).
  * **Comparing Two Users:**
      * Uses the `--compare-user USER1 USER2` flag.
      * Does **not** require a `USERNAME` argument at the beginning of the command (it takes the usernames from the flag).
      * Loads the *existing* statistical JSON data for both users from your output directory (it does *not* generate new stats on the fly unless the data files are missing).
      * Generates a side-by-side comparison report.
      * Filters (`--start-date`, etc.) are ignored in this mode, as it compares the full dataset available in the stats files.
  * **Monitoring a User:**
      * Uses the `--monitor USERNAME` flag.
      * Requires a `USERNAME` argument.
      * Enters a continuous loop, periodically checking for new activity for the specified user.
      * When new activity is detected, it scrapes, updates the JSON, and exports CSVs.
      * Does *not* automatically generate stats or run AI analysis on each update by default (this could be added as a future feature or custom modification).
      * Uses the `monitor_interval_seconds` config setting or the `--monitor-interval` flag.
  * **Generating AI Prompts:**
      * Uses the `--generate-prompt` flag.
      * Does **not** require a `USERNAME` argument.
      * Starts an interactive chat session with the Gemini API to help you refine a system prompt for AI analysis.
      * Saves the resulting prompt to a file in the `default_prompt_dir`.
  * **Exporting JSON Data Only:**
      * Uses the `--export-json-only USERNAME` flag.
      * Requires a `USERNAME` argument.
      * Performs the scraping/updating process and saves the `username.json` file.
      * Exports the filtered data to `username-posts.csv` and `username-comments.csv`.
      * Exits without generating stats or running AI analysis. Useful for just keeping your data files updated.

### Examples

Here are practical examples demonstrating common use cases:

1.  **Scrape/Update data for `SomeRedditor`, generate their stats report, and run AI analysis using the default prompt:**

    ```bash
    python redstalk.py SomeRedditor --generate-stats --run-analysis
    ```

2.  **Generate Stats for `UserX`, filtering activity between specific dates:**

    ```bash
    python redstalk.py UserX --generate-stats --start-date 2023-01-01 --end-date 2023-12-31
    ```

    *(Note: Dates should be in YYYY-MM-DD format and are interpreted as UTC.)*

3.  **Generate Stats for `UserY`, focusing only on posts/comments in `r/python` and `r/datascience`:**

    ```bash
    python redstalk.py UserY --generate-stats --focus-subreddit python datascience
    ```

    *(Provide subreddit names without the `r/` prefix).*

4.  **Generate Stats for `UserZ`, excluding activity from `r/politics` and `r/news`:**

    ```bash
    python redstalk.py UserZ --generate-stats --ignore-subreddit politics news
    ```

5.  **Combine date and subreddit filters for `AnalystBob`, analyzing recent activity in specific investment subreddits while excluding news:**

    ```bash
    python redstalk.py AnalystBob --generate-stats --start-date 2024-01-01 --focus-subreddit investing stocks wallstreetbets --ignore-subreddit news worldnews
    ```

6.  **Run AI Analysis on `CreativeWriter`'s *filtered* data (using default prompt and `raw` mode):**

    ```bash
    python redstalk.py CreativeWriter --run-analysis --analysis-mode raw
    ```

7.  **Run AI Analysis on `TechGuru`'s *filtered* data using a custom prompt file and `mapped` mode, fetching external post titles for context:**

    ```bash
    python redstalk.py TechGuru --run-analysis --prompt-file prompts/tech_analysis_prompt.txt --analysis-mode mapped --fetch-external-context
    ```

8.  **Generate Stats and AI Analysis for `HelpfulUser`, and include an AI-generated summary of the stats report:**

    ```bash
    python redstalk.py HelpfulUser --generate-stats --run-analysis --summarize-stats
    ```

    *(Requires both `--generate-stats` and `--run-analysis` for `--summarize-stats` to work).*

9.  **Compare the statistical reports of `UserA` and `UserB`:**

    ```bash
    python redstalk.py --compare-user UserA UserB
    ```

    *(This will look for existing `_stats_data.json` or Markdown stats files for both users in your output directory.)*

10. **Monitor `NewsJunkie` for new activity, checking every 10 minutes (600 seconds):**

    ```bash
    python redstalk.py NewsJunkie --monitor --monitor-interval 600
    ```

11. **Start the interactive prompt generator to create a new AI system prompt:**

    ```bash
    python redstalk.py --generate-prompt
    ```

12. **Only scrape and update the JSON and CSV files for `DataHoarder`, without generating reports:**

    ```bash
    python redstalk.py DataHoarder --export-json-only
    ```

13. **Force a complete re-scrape of all activity for `OldProfile`, ignoring any existing JSON data, and then generate stats:**

    ```bash
    python redstalk.py OldProfile --generate-stats --force-scrape
    ```

### Key Options Detailed

  * `USERNAME`: The Reddit username to process (required for single-user actions and monitoring).
  * `--generate-stats`: Flag to trigger the statistical analysis and report generation.
  * `--run-analysis`: Flag to trigger the AI-powered textual analysis.
  * `--compare-user USER1 USER2`: Exclusive action flag to compare two users' statistics. Requires providing the two usernames.
  * `--monitor USERNAME`: Exclusive action flag to start monitoring a user for new activity. Requires the username as an argument to this flag.
  * `--generate-prompt`: Exclusive action flag to start the interactive AI prompt generator.
  * `--export-json-only`: Exclusive action flag to only scrape/update data files (JSON and CSV) and exit. Requires a USERNAME argument.
  * `--reset-config`: Exclusive action flag to reset the `config.json` file to defaults.
  * `--output-dir DIR`: Specify a different base directory for all output files. Overrides the `default_output_dir` in `config.json`.
  * `--log-level LEVEL`: Set the verbosity of the logging output. Options include DEBUG, INFO, WARNING, ERROR, CRITICAL. Default: INFO.
  * `--start-date YYYY-MM-DD`: Filter activity to include only posts/comments created on or after this date (UTC).
  * `--end-date YYYY-MM-DD`: Filter activity to include only posts/comments created on or before this date (UTC).
  * `--focus-subreddit SUB1 SUB2 ...`: Provide a space-separated list of subreddit names. Only include activity within these subreddits.
  * `--ignore-subreddit SUB1 SUB2 ...`: Provide a space-separated list of subreddit names. Exclude activity within these subreddits.
  * `--force-scrape`: Ignore existing `username.json` data and re-download all available posts and comments from the Reddit API. Useful if you suspect data corruption or want to ensure you have the absolute latest data.
  * `--scrape-comments-only`: When scraping, only fetch comments, skipping posts.
  * `--prompt-file FILE`: Specify the path to a custom system prompt file for AI analysis. Overrides `default_prompt_file`.
  * `--api-key KEY`: Provide the Google Gemini API key directly on the command line. This has the highest priority but is the least secure method.
  * `--analysis-mode MODE`: Choose the mode for AI analysis: `mapped` (groups comments under posts for context) or `raw` (sequential list of all activities). Default: `mapped`.
  * `--fetch-external-context`: In `mapped` analysis mode, attempt to fetch titles for posts that link to external websites. This provides the AI with more context about the linked content. Requires extra API calls.
  * `--summarize-stats`: If `--generate-stats` is also used, trigger an AI summary of the generated statistics report.
  * `--top-words N`: Specify the number of top words and n-grams to include in the statistical report's frequency lists. Default: 50.
  * `--top-items N`: Specify the number of top/bottom posts/comments to list in the statistical report (based on upvotes/engagement). Default: 10.
  * `--stats-output-json FILE`: Save the calculated statistics data (before formatting into Markdown) as a JSON file. This is useful if you want to programmatically access the statistics. By default, saves to `username_stats_data.json` in the user's directory.

## Statistical Analysis (`stats` module)

The `stats` package is a core component for quantitative analysis. It takes the filtered user data (from CSVs or JSON) and calculates a broad range of metrics using `calculations.py`. These results are then formatted into a detailed Markdown report by `reporting.py`. The comparison report (`comparison.py`) leverages these individual stats.

### Available Metrics

The statistical report covers several categories:

  * **Basic Information:** Username, time range of activity, account creation date, karma breakdown (link/comment), total posts/comments.
  * **Subreddit Engagement:**
      * Total unique subreddits participated in.
      * List of top subreddits by activity count.
      * Diversity indices (e.g., Shannon index) to measure how spread out activity is across subreddits.
      * Activity breakdown by subreddit (counts, percentage).
  * **Content Analysis:**
      * Total word count in posts and comments.
      * Lexical diversity (Type-Token Ratio) as an indicator of vocabulary richness.
      * Breakdown of post types (self-text, link).
      * Estimated ratio of questions asked (using NLTK).
  * **Engagement Metrics:**
      * Average upvotes per post/comment.
      * Total/Average awards received.
      * Average comments per post.
      * Lists of top and bottom performing posts and comments (by upvotes).
  * **Metadata Analysis:**
      * Analysis of flair usage (text, CSS class).
      * Frequency of edited posts/comments.
      * Estimated percentage of deleted content.
      * Frequency of crossposting.
  * **Temporal Patterns:**
      * Activity distribution by hour of the day, day of the week, month, and year.
      * Analysis of posting "burstiness".
  * **Simple NLP:**
      * Most frequent words and n-grams (sequences of words) in posts and comments.
      * VADER sentiment analysis: Ratio of positive, negative, and neutral sentiment; sentiment scores over time (sentiment arc).
      * Frequency of mentions of other users (`u/`).

## AI-Powered Analysis (`analysis` module)

The `analysis` module interacts with the Google Gemini API to perform qualitative, narrative analysis based on the user's text content. This allows for generating character profiles, identifying recurring themes, analyzing writing style, or performing other tasks specified by the system prompt.

The process involves:

1.  Loading the filtered post and comment data.
2.  Formatting the data appropriately based on the chosen `--analysis-mode`.
3.  Using the Gemini API's token counter to determine how to chunk the data to stay within the model's context window.
4.  Sending each chunk to the Gemini model along with the system prompt.
5.  Collecting the AI's responses for each chunk.
6.  Combining the responses (if applicable and desired by the prompt) and saving the final analysis report.

### Analysis Modes (`mapped` vs `raw`)

  * **`mapped` Mode (Default):** This mode attempts to provide the AI with more conversational context. It groups comments under their parent posts. The structure sent to the AI looks roughly like:
    ```
    ## Post Title (or External URL)
    Post Body
    > Comment 1
    >> Reply to Comment 1
    > Comment 2
    ...
    ```
    This can be more effective for prompts that ask the AI to analyze interactions or discussions. Use `--fetch-external-context` to provide the AI with the title of linked external content.
  * **`raw` Mode:** This mode provides the AI with a simple chronological list of the user's posts and comments, without explicit grouping. The structure is just a sequence of text entries:
    ```
    Post 1 Title
    Post 1 Body
    Comment 1 Text
    Post 2 Title
    ...
    ```
    This might be better for prompts focused purely on writing style, overall themes regardless of context, or when the post-comment relationship is less important.

### Prompt Engineering

The effectiveness of the AI analysis heavily relies on the quality of the system prompt. A good prompt clearly instructs the AI on its role, the task to perform, the desired output format, and any constraints.

The `--generate-prompt` flag launches an interactive assistant that chats with the Gemini API to help you write and refine a system prompt. It guides you through specifying the AI's persona, the user data context, the analytical goals, and formatting requirements. This is highly recommended if you are new to writing system prompts or want to experiment with different analytical approaches.

## Output Files

RedStalk organizes output files within the `default_output_dir` (or the directory specified by `--output-dir`), typically creating a subdirectory for each analyzed username.

  * `/[output_dir]/[username]/[username].json`: Contains the raw scraped JSON data (posts and comments).
  * `/[output_dir]/[username]/[username]-posts.csv`: CSV file containing filtered post data.
  * `/[output_dir]/[username]/[username]-comments.csv`: CSV file containing filtered comment data.
  * `/[output_dir]/[username]/[username]_stats_YYYYMMDD_HHMMSS.md`: Markdown report of the statistical analysis for a single user. The timestamp helps distinguish multiple reports for the same user.
  * `/[output_dir]/[username]/[username]_stats_data.json` (Optional): JSON file containing the raw calculated statistics (if `--stats-output-json` is used).
  * `/[output_dir]/[username]/[username]_charc_[mode]_YYYYMMDD_HHMMSS.md`: Markdown report of the AI analysis results. `[mode]` will be either `mapped` or `raw` depending on the analysis mode used.
  * `/[output_dir]/comparison_[USER1]_vs_[USER2]_YYYYMMDD_HHMMSS.md`: Markdown report comparing the statistics of two users. Saved directly in the base output directory.
  * `/[output_dir]/prompts/`: Default directory for saving custom system prompt files generated by `--generate-prompt`.

## How It Works (High Level Architecture)

1.  **Initialization (`redstalk.py`):** Parses command-line arguments, loads configuration from `config.json`, sets up logging.
2.  **Action Dispatch (`redstalk.py`):** Based on the arguments, determines the primary action(s) to perform (scrape, stats, analysis, compare, monitor, generate prompt, reset config, export only).
3.  **Data Fetching (`reddit_utils.py`):**
      * Handles interaction with the Reddit API using the `requests` library and the configured `user_agent`.
      * Takes a username and retrieves their posts and comments chronologically.
      * Implements incremental scraping: reads the existing `.json` file to find the timestamp of the latest activity and only fetches newer entries from Reddit, reducing API calls and processing time on subsequent runs.
      * Merges newly fetched data with existing data and saves the updated dataset to `username.json`.
      * Handles API rate limits gracefully with retries.
4.  **Data Preparation (`data_utils.py`):**
      * Loads the full dataset from `username.json`.
      * Applies filtering logic based on `--start-date`, `--end-date`, `--focus-subreddit`, and `--ignore-subreddit`.
      * Extracts the filtered data into separate lists for posts and comments.
      * Exports these filtered lists to `username-posts.csv` and `username-comments.csv`.
5.  **Statistics Generation (`stats/` package):**
      * Instantiated when `--generate-stats` or `--compare-user` is used.
      * Reads filtered data (preferably from CSVs for efficiency in some calculations).
      * `calculations.py`: Contains functions to compute all the defined statistical metrics from the data. Requires optional dependencies like `vaderSentiment`, `nltk`, and `pandas`.
      * `reporting.py`: Takes the calculated statistics and formats them into a structured Markdown report for a single user.
      * `comparison.py`: Loads statistics for two users (either from pre-calculated JSON or by triggering `calculations.py`), computes comparative metrics (like overlap), and generates a side-by-side Markdown comparison report.
6.  **AI Analysis Execution (`analysis.py`, `ai_utils.py`):**
      * Instantiated when `--run-analysis` or `--summarize-stats` is used.
      * Reads filtered data (from CSVs).
      * `ai_utils.py`: Contains core logic for interacting with the Google Gemini API.
          * `chunk_items`: Splits the data into manageable chunks based on the target token size (`default_chunk_size`) and the model's maximum context window, using the API's token counting endpoint.
          * `perform_ai_analysis`: Sends a chunk of data and the system prompt to the specified Gemini model and retrieves the response. Handles API calls and potential errors.
      * `analysis.py`: Orchestrates the AI analysis process. Reads the prompt file, structures the data based on the analysis mode (`mapped` or `raw`), calls `ai_utils` to process chunks, and combines the results into the final Markdown report. Includes logic for `--fetch-external-context` and `--summarize-stats`.
7.  **Monitoring Loop (`monitoring.py`):**
      * Instantiated when `--monitor` is used.
      * Enters a loop that periodically calls the scraping function (`reddit_utils.py`) to check for new activity based on the specified interval.
      * If new activity is found, it triggers the data saving and CSV export steps (`data_utils.py`).
8.  **Prompt Generation (`ai_utils.py`):**
      * Triggered by `--generate-prompt`.
      * Initiates an interactive chat session using the Gemini API, guiding the user through prompt creation and refinement.
      * Saves the final prompt text to a file.

## Dependencies

  * **Core Functionality:**
      * `requests`: Essential for all HTTP communication with the Reddit API.
      * `rich`: Provides enhanced, colored terminal output and progress bars for a better user experience.
  * **AI Features (Optional):**
      * `google-generativeai`: The official Python client library for interacting with the Google Gemini API. Required for any AI-related operations.
      * `tqdm`: Used to display progress bars during token counting for AI analysis, providing visual feedback on processing large datasets.
  * **Statistics Features (Optional):**
      * `vaderSentiment`: A lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media. Used for sentiment metrics in the stats report.
      * `nltk`: The Natural Language Toolkit. Used for tokenization and other basic NLP tasks, specifically for estimating the ratio of questions in content. Requires downloading the `punkt` data.
      * `pandas`: A powerful data manipulation library. Used in some statistical calculations (particularly those involving CSVs) for efficient data handling and analysis.

## Contributing

We welcome contributions to RedStalk\! Whether it's reporting a bug, suggesting a new feature, improving documentation, submitting code via a pull request, or making the current vibe coded bullshit better, (we need this the most), your help is appreciated.

1.  **Fork the repository:** Fork the `redstalk` repository on GitHub.
2.  **Clone your fork:** Clone your forked repository to your local machine.
3.  **Create a new branch:** Create a new branch for your feature or bugfix (`git checkout -b my-new-feature`).
4.  **Make your changes:** Implement your changes and test them thoroughly.
5.  **Write documentation:** Update the README.md or add specific documentation for your changes.
6.  **Commit your changes:** Commit your changes with clear and concise commit messages.
7.  **Push to your fork:** Push your changes to your fork on GitHub (`git push origin my-new-feature`).
8.  **Open a Pull Request:** Open a pull request from your fork to the main `redstalk` repository, describing your changes and their purpose.

Please ensure your code adheres to the project's stylings (if you really think they exist) and that all tests pass.

## License (COPYLEFT FTW)

This project is licensed under the GNU General Public License v3.0 (GPL v3).

The GPL v3 is a free, copyleft license. The main clauses include:

* **Freedom to Run:** You are free to run the program for any purpose.
* **Freedom to Study and Change:** You are free to study how the program works and change it. Access to the source code is a necessary condition for this.
* **Freedom to Redistribute:** You are free to redistribute copies of the program.
* **Freedom to Distribute Modified Versions:** You are free to distribute copies of your modified versions to others.

When distributing the software, or any work based on it, you must do so under the same license (GPL v3), include the source code, and provide recipients with the same four freedoms.

See the [LICENSE](LICENSE) file in the repository root for the full license text.

## Disclaimer and Ethical Considerations

  * **Reddit API Terms of Service:** This tool interacts with the public Reddit API. Your use of RedStalk **must comply** with the [Reddit API Terms of Service](https://www.redditinc.com/policies/developer-terms). Pay close attention to the requirements for User-Agents and rate limits.
  * **User-Agent:** As reiterated multiple times, setting a unique and descriptive `user_agent` including your Reddit username is **MANDATORY** per Reddit's terms. Failure to do so can lead to issues.
  * **Ethical Use:** This tool processes publicly available data. However, analyzing and potentially profiling individuals based on their online activity raises significant ethical considerations.
      * **Respect Privacy:** Even though data is public, consider the privacy implications of your analysis. Do not use this tool for doxxing, harassment, stalking, or any malicious purposes.
      * **Transparency:** If you are using this tool for research or public presentation, be transparent about your methods and the data sources.
      * **Consent:** If your analysis involves individuals and you plan to present findings in a way that could identify them, consider if consent is necessary or appropriate.
  * **AI Accuracy:** AI models can hallucinate or provide inaccurate analysis. The quality of AI output depends heavily on the input data, the model used, and the system prompt. Do not rely solely on AI analysis for critical decisions.
  * **No Liability:** The developers of RedStalk (which happen to be I, me and myself and all other open source contributors *in the future*) assume no responsibility or liability for any misuse of this tool, any consequences arising from its use, or any violations of Reddit's terms or ethical guidelines by users. You are solely responsible for your actions when using RedStalk.

**Use this tool responsibly and ethically.**

## Troubleshooting

  * **`ModuleNotFoundError`:** Ensure you have activated your virtual environment (`source venv/bin/activate` or `venv\Scripts\activate`) and installed all dependencies (`pip install -r requirements.txt`).
  * **Reddit API Rate Limits:** If you encounter frequent errors related to rate limits, this means you are making too many requests in a short period.
      * Ensure your `user_agent` is set correctly in `config.json`.
      * Incremental scraping is designed to minimize this, but `--force-scrape` will make many requests. Use it sparingly.
      * Reddit's API policies and rate limits can change. Refer to the official Reddit API documentation.
  * **Gemini API Errors:**
      * Check that your API key is set correctly (environment variable, `config.json`, or `--api-key`).
      * Ensure you have enabled the Gemini API in your Google Cloud project or Google AI Studio.
      * Check the model name in `config.json` (`default_model_name`). Ensure it's a valid model name available to you.
      * If you get errors related to token limits even with chunking, you might need to reduce the `default_chunk_size` or use a model with a larger context window.
  * **Incorrect Filtering:** Double-check the format of your dates (`YYYY-MM-DD`) and subreddit names (no `r/`) when using filtering flags.
  * **Configuration Not Applying:** Remember the priority order for settings: Command Line Flags \> Environment Variables \> `config.json`. Ensure the setting you are trying to change isn't being overridden by a higher-priority method.

If you encounter persistent issues, please open an issue on the GitHub repository with details about the problem, your operating system, Python version, RedStalk version, and the command you were running.

## Future Enhancements

Possible future features and improvements for RedStalk include:

1.  **Sentiment Arc:** (Simple NLP) - Data: Filtered JSON (text, time) - Calc average sentiment over time windows using VADER - Output: Sentiment trend over time (table/graph data).
2.  **Question Asking Ratio:** (Simple NLP) - Data: Filtered CSV (text) - Count items containing questions using NLTK sentence tokenization - Output: Ratio of items containing questions.
3.  **Mention Frequency:** (Simple NLP) - Data: Filtered CSV (text) - Find and count `u/` and `r/` mentions using regex - Output: List of most frequent user and subreddit mentions.
4.  **AI Subreddit Categorization:** (Potential AI) - Data: List of subreddits - Use AI (Gemini) to categorize subreddits based on name - Output: Subreddit-to-category mapping and category counts.
5.  **AI Topic Modeling / Keyword Extraction:** (Potential AI) - Data: Filtered CSV (text) - Use AI (Gemini) to extract keywords/topics from text chunks and aggregate - Output: List of most frequent topics/keywords.
6.  **AI Behavioral Trait Identification:** (Potential AI) - Data: Filtered CSV (text) - Use AI (Gemini) with specific prompts to analyze text for traits (e.g., helpfulness) - Output: AI summary/score for requested traits.
7.  **Reply Depth Analysis:** (Deeper Interaction) - Data: Filtered JSON (permalink) - Parse permalinks to estimate comment depth relative to post URL - Output: Average and max comment depth.
8.  **Content Originality Estimate:** (Deeper Interaction) - Data: Filtered JSON (post type, score, comments) - Compare engagement metrics (score, comments) between self posts and link posts - Output: Average score/comments by post type.
9.  **Interactive Console Stats:** (Utility) - Data: Final stats dictionary - Display stats dictionary interactively in the console using `rich` - Output: Formatted console output (no file).
10. **Sentiment Trajectory Analysis:** (Advanced) - Data: Filtered JSON (text, time, subreddit) - Analyze sentiment trend (overall, per-subreddit) using VADER - Output: Overall trend, per-subreddit sentiment comparison.
11. **Linguistic Style Markers & Consistency:** (Advanced) - Data: Filtered CSV (text, subreddit) - Calculate metrics (words/sent, punctuation, caps, emoji, markdown) per item and per-subreddit consistency - Output: Overall style averages, cross-subreddit consistency summary.
12. **Interaction Role Tendencies:** (Advanced AI Assist) - Data: Filtered CSV (text, subreddit) - Use AI (Gemini) to estimate primary role tendencies within top subreddits - Output: AI-estimated roles/tendencies per top subreddit.
13. **Subreddit Persona Comparison:** (Advanced AI Assist) - Data: Filtered CSV (text, subreddit) - Run AI character analysis separately for text from each of top N subreddits and compare - Output: Comparative AI analysis summaries for top N subreddits.
14. **"Rabbit Hole" / Topic Burst Detector:** (Advanced) - Data: Filtered JSON (time, subreddit) - Detect periods of concentrated activity within a single subreddit over a short time window - Output: List of detected subreddit activity bursts.

Feel free to suggest new ideas by opening an issue on the GitHub repository\!
